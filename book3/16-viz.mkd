Wizualizacja danych
===================

Nauczyliśmy się pewnych aspektów Pythona, sprawdzaliśmy jak go używać Pythona oraz jak korzystać z sieci i baz danych w celu zarządzania danymi.

W poniższym rozdziale przyjrzymy się trzem kompletnym aplikacjom, które łączą wszystkie te rzeczy razem, tak aby zarządzać danymi i je wizualizować. Możesz później użyć tych aplikacji jako przykładowego kodu, który pomoże Ci rozpocząć rozwiązywanie Twojego problemu.

Każda z tych aplikacji jest plikiem ZIP, który możesz pobrać na swój komputer, rozpakować i uruchomić.

Budowanie mapy OpenStreetMap z geokodowanych danych
---------------------------------------------------

\index{OpenStreetMap!map}
\index{Visualization!map}

W tym projekcie wykorzystamy API geokodowania OpenStreetMap (usługa Nominatim), po to by zamienić wpisane przez użytkowników nazwy uczelni na lokalizacje geograficzne^[Należy mieć na uwadze, że taka zamiana tekstu na dane geolokalizacyjne nie zawsze daje precyzyjne i poprawne wyniki, co będzie można też zauważyć w części wyników tej aplikacji.], a następnie umieścimy przetworzone dane na mapie OpenStreetMap. 

![Mapa OpenStreetMap](../images/osm-map)

Aby rozpocząć, pobierz aplikację z poniższego adresu:

[pl.py4e.com/code3/geodata.zip](https://pl.py4e.com/code3/geodata.zip)

W warunkach korzystania z usługi Nominatim jest wskazanie by ogarniczyć się do maksymalnie jednego zapytania na sekundę (usługa jest darmowa, stąd też gdybyśmy generowali bardzo dużą liczbę zapytań w krótkim czasie, to prawdopodobnie szybko zablokowano by nam dostęp do API). Nasze zadanie dzielimy na dwie fazy.

\index{cache}

W pierwszej fazie bierzemy nasze dane wejściowe z pliku *where.data* i odczytujemy je wiersz po wierszu, odczytując przy tym zgeokodowaną odpowiedź serwera Nominatim i przechowujemy ją w bazie danych (plik *geodata.sqlite*). Zanim użyjemy API geokodowania, po prostu sprawdzamy, czy w naszej bazie ("pamięci podręcznej") mamy już dane dla tego konkretnego wiersza, dzięki czemu w przypadku ponownego uruchomienia programu nie będziemy musieli drugi razy wysyłać zapytania do API.

W dowolnym momencie możesz uruchomić cały proces od początku, po prostu usuwając wygenerowany plik *geodata.sqlite*.

Uruchom program *geoload.py*. Program ten odczyta wiersze wejściowe z pliku *where.data* i dla każdego wiersza sprawdzi, czy jest on już w bazie danych, a jeśli nie mamy danych dla przetwarzanej lokalizacji, to wywoła on zapytanie API geokodowania aby pobrać dane i przechowywać je w bazie SQLite.

Oto przykładowe uruchomienie po tym, jak w bazie danych znajdują się już jakieś dane:

~~~~
Znaleziono w bazie  AGH University of Science and Technology

Znaleziono w bazie  Academy of Fine Arts Warsaw Poland

Znaleziono w bazie  American University in Cairo

Znaleziono w bazie  Arizona State University

Znaleziono w bazie  Athens Information Technology

Pobieranie https://nominatim.openstreetmap.org/search.php?q=University+of+Pretoria&format=geojson&limit=1&addressdetails=1&accept-language=pl
Pobrano 954 znaków {"type":"FeatureColl

Pobieranie https://nominatim.openstreetmap.org/search.php?q=University+of+Salamanca&format=geojson&limit=1&addressdetails=1&accept-language=pl
Pobrano 822 znaków {"type":"FeatureColl

...
~~~~

Pierwsze pięć lokalizacji znajduje się już w bazie danych, a więc są one pomijane. Program przetwarza dane do momentu, w którym znajdzie niezapisane lokalizacje i zaczyna o nie odpytywać API.

Plik *geoload.py* może zostać zatrzymany w dowolnym momencie, a ponadto kod zawiera licznik (zmienna *count*), którego można użyć do ograniczenia liczby połączeń do API geokodowania w danym uruchomieniu programu.

Po załadowaniu danych do *geodata.sqlite*, możesz je zwizualizować za pomocą programu *geodump.py*. Program ten odczytuje bazę danych i zapisuje plik *where.js* zawierający lokalizacje, szerokości i długości geograficzne w postaci wykonywalnego kodu JavaScript. Pobrany przez Ciebie plik ZIP zawiera już wygenerowany *where.js*, ale możesz go wygenerować jeszcze raz aby sprawdzić działanie programu *geodump.py*.

Uruchomienie programu *geodump.py* odbywa się w następujący sposób:

~~~~
Akademia Górniczo-Hutnicza ... Polska 50.065703299999996 19.918958667058632
Akademia Sztuk Pięknych ... Polska 52.2397515 21.015564130658333
...
260 wierszy zapisano do where.js
Otwórz w przeglądarce internetowej plik where.html aby obejrzeć dane.
~~~~

Plik *where.html* składa się z kodu HTML i JavaScript, które służą do wizualizacji mapy OpenStreetMap przy pomocy biblioteki OpenLayers. Strona odczytuje najświeższe dane z pliku *where.js* po to by uzyskać dane niezbędne do wizualizacji. Oto format pliku *where.js*:

~~~~ {.js}
myData = [
[50.065703299999996,19.918958667058632, 'Akademia Górniczo-Hutnicza, ... , Polska'],
[52.2397515,21.015564130658333, 'Akademia Sztuk Pięknych, ... , Polska'],
   ...
];
~~~~

Jest to lista list zapisana w języku JavaScript. Składnia listy w języku JavaScript jest bardzo podobna do składni Pythona.

By zobaczyć lokalizacje na mapie, otwórz plik *where.html* w przeglądarce internetowej. Możesz najechać kursorem na każdą pinezkę mapy i na nią kliknąć, tak aby znaleźć lokalizację, którą zwróciło API kodowania dla danych wejściowych wprowadzonych przez użytkownika. Jeżeli po otwarciu pliku *where.html* nie widzisz żadnych danych, sprawdź czy w przeglądarce jest włączony JavaScript lub w konsoli deweloperskiej swojej przeglądarki sprawdź czy są jakieś błędy. 

Wizualizacja sieci i połączeń
-----------------------------

\index{Google!page rank}
\index{Visualization!networks}
\index{Visualization!page rank}

W poniższej aplikacji będziemy wykonywać niektóre funkcje znajdujące się w mechaniźmie wyszukiwarki internetowej. Najpierw przeczeszemy mały fragment sieci internetowej, uruchomimy uproszczoną wersję algorytmu Google PageRank by określić strony, do których jest najwięcej odniesień z innych stron (czyli by wskazać które strony są potencjalnie najistotniejsze), a następnie zwizualizujemy rangę strony i połączenia w naszym małym zakątku sieci. Aby utworzyć wizualizacje wykorzystamy bibliotekę JavaScript D3 <https://d3js.org/>.

Możesz pobrać i rozpakować poniższą aplikację:

[pl.py4e.com/code3/pagerank.zip](https://pl.py4e.com/code3/pagerank.zip)

![Algorytm PageRank](height=3.5in@../images/pagerank)

Pierwszy program (*spider.py*) wczytuje stronę internetową i umieszcza serię stron do bazy danych (*spider.sqlite*), rejestrując przy tym odnośniki (linki) pomiędzy stronami. W każdej chwili możesz zrestartować cały proces, usuwając plik *spider.sqlite* i uruchamiając ponownie *spider.py*. 

~~~~
Wpisz adres internetowy lub wciśnij Enter:   
['https://www.dr-chuck.com']
Ile stron: 25
1 https://www.dr-chuck.com (8386) 4
4 https://www.dr-chuck.com/dr-chuck/resume/index.htm (1855) 9
12 https://www.dr-chuck.com/dr-chuck/resume/pictures/index.htm (1827) 5
...
Ile stron: 
~~~~

W powyższym przykładowym uruchomieniu wskazaliśmy naszemu robotowi, by sprawdził domyślną stronę i pobrał 25 stron. Jeśli zrestartujesz program i wskażesz by przeszukał więcej stron, to nie będzie on ponownie przeszukiwał stron już znajdujących się w bazie danych. Po ponownym uruchomieniu robot przechodzi do losowej, niesprawdzonej jeszcze strony i zaczyna tam swoją pracę. Tak więc każde kolejne uruchomienie *spider.py* jest dodaje tylko nowe strony. 

~~~~
Wpisz adres internetowy lub wciśnij Enter: 
Kontynuowanie istniejącego indeksowania stron. Usuń spider.sqlite, aby rozpocząć nowe indeksowanie.
['https://www.dr-chuck.com']
Ile stron: 3
22 https://www.dr-chuck.com/csev-blog/category/uncategorized (91096) 61
27 https://www.dr-chuck.com/csev-blog/2014/09/how-to-achieve-vendor-lock-in-with-a-legit-open-source-license-affero-gpl (59001) 20
30 https://www.dr-chuck.com/csev-blog/2020/08/styling-tsugi-koseu-lessons (33522) 21
Ile stron:
~~~~

Możesz mieć wiele punktów startowych w tej samej bazie danych - w programie nazywane są one "witrynami". Robot internetowy jako następną stronę do sprawdzenia wybiera losową stronę spośród wszystkich nieodwiedzonych linków na wszystkich witrynach.

Jeżeli chcesz wyświetlić zawartość bazy *spider.sqlite*, możesz uruchomić *spdump.py*: 

~~~~
(16, None, 1.0, 2, 'https://www.dr-chuck.com/csev-blog')
(15, None, 1.0, 30, 'https://www.dr-chuck.com/csev-blog/2020/08/styling-tsugi-koseu-lessons')
(15, None, 1.0, 22, 'https://www.dr-chuck.com/csev-blog/category/uncategorized')
...
22 wierszy.
~~~~

Program dla danej strony pokazuje liczbę przychodzących linków, stary współczynnik PageRank, nowy współczynnik PageRank, id strony oraz adres URL. Program *spdump.py* pokazuje tylko te strony, które mają co najmniej jedno odniesienie z innych stron.

Gdy będziesz już miał kilka stron w swojej bazie danych, to za pomocą programu *sprank.py* możesz uruchomić algorytm obliczania współczynnika PageRank. Musisz tylko podać ile iteracji algorytmu program ma wykonać. 

~~~~
Ile iteracji: 2
1 0.720326643053916
2 0.34992366601870745
[(1, 0.6196280991735535), (2, 2.4944679374657728), (3, 0.6923553719008263), (4, 1.1014462809917351), (5, 0.2696280991735535)]
~~~~

Możesz ponownie wyświetlić zawartość bazy aby zobaczyć, że współczynnik PageRank dla stron został zaktualizowany: 

~~~~
(16, 1.0, 2.4944679374657728, 2, 'https://www.dr-chuck.com/csev-blog')
(15, 1.0, 2.1360764832409846, 30, 'https://www.dr-chuck.com/csev-blog/2020/08/styling-tsugi-koseu-lessons')
(15, 1.0, 2.449518442516278, 22, 'https://www.dr-chuck.com/csev-blog/category/uncategorized')
...
22 wierszy.
~~~~

Możesz uruchamiać *sprank.py* tyle razy, ile chcesz, a program po prostu poprawi obliczenie współczynnika PageRank za każdym razem, gdy go uruchomisz. Możesz nawet uruchomić *sprank.py* kilka razy, a następnie dodać kilka kolejnych stron poprzez *spider.py*, a następnie znów uruchomić *sprank.py*, by dalej przeliczyć wartości PageRank. Wyszukiwarka internetowa zazwyczaj przez cały czas uruchamia zarówno programy do indeksowania stron, jak i do tworzenia rankingu.

Jeżeli chcesz uruchomić obliczenia PageRank od początku bez ponownego przejścia robotem po stronach, to możesz użyć programu *spreset.py*, a następnie możesz uruchomić ponownie *sprank.py*.

Wynik uruchomienia *spreset.py*:

~~~~
Wszystkie strony mają ustawiony współczynnik PageRank na 1.0
~~~~

Ponowne uruchomienie *sprank.py*:

~~~~
Ile iteracji: 50
1 0.720326643053916
2 0.34992366601870745
3 0.17895552923503424
4 0.11665048143652895
...
46 3.579258334100184e-05
47 3.0035450290624035e-05
48 2.520367345856324e-05
49 2.114963873141301e-05
50 1.7747381915049988e-05
[(1, 9.945248881666563e-05), (2, 3.205252622657907), (3, 0.00012907931109952867), (4, 0.0003719906004627465), (5, 9.966636303771006e-05)]
~~~~

Dla każdej iteracji algorytmu PageRank program wypisuje średnią zmianę współczynnika na stronę. Początkowo sieć jest dość niezbalansowana, więc poszczególne wartości rankingu bardzo się zmieniają pomiędzy kolejnymi iteracjami. Jednak w kilku kolejnych iteracjach algorytm zbiega szybko do końcowego wyniku. Powinieneś uruchomić *sprank.py* na tyle długo, by kolejne wartości generowane przez aglorytm nie miały już zbyt dużych różnic.

Jeśli chcesz zwizualizować strony, które aktualnie najwyżej znajdują się w rankingu, uruchom program *spjson.py*. Odczytuje on bazę danych i zapisuje dane dotyczące najbardziej linkowanych stron w formacie JSON, który może być obejrzany w przeglądarce internetowej. 

~~~~
Tworzenie JSONa w pliku spider.js...
Ile węzłów? 30
Otwórz force.html w przeglądarce internetowej by zobaczyć wizualizację
~~~~

Możesz obejrzeć wynik otwierając plik *force.html* w swojej przeglądarce internetowej. Pokazuje on automatyczny układ węzłów (stron) i połączeń między nimi. Możesz kliknąć i przeciągnąć dowolny węzeł, a także dwukrotnie kliknąć na węzeł, by wyświetlić adres URL, który jest reprezentowany przez ten węzeł. Wielkość węzła reprezentuje jego istotność, tzn. że wiele innych stron do linkuje do tej strony.

Jeżeli uruchomisz ponownie inne narzędzia tej aplikacji, uruchom ponownie *spjson.py* i ośwież stronę *force.html* w przeglądarce, tak aby uzyskać nowe dane umieszczone w *spider.json*.

Visualizing mail data
---------------------

Up to this point in the book, you have become quite familiar with our
*mbox-short.txt* and *mbox.txt* data
files. Now it is time to take our analysis of email data to the next
level.

In the real world, sometimes you have to pull down mail data from
servers. That might take quite some time and the data might be
inconsistent, error-filled, and need a lot of cleanup or adjustment. In
this section, we work with an application that is the most complex so
far and pull down nearly a gigabyte of data and visualize it.

![A Word Cloud from the Sakai Developer List](height=3.5in@../images/wordcloud)

You can download this application from:

[www.py4e.com/code3/gmane.zip](http://www.py4e.com/code3/gmane.zip)

We will be using data from a free email list archiving service called
[www.gmane.org](http://www.gmane.org). This service is very popular with open
source projects because it provides a nice searchable archive of their
email activity. They also have a very liberal policy regarding accessing
their data through their API. They have no rate limits, but ask that you
don't overload their service and take only the data you need. You can
read gmane's terms and conditions at this page:

<http://gmane.org/export.php>

*It is very important that you make use of the gmane.org data
responsibly by adding delays to your access of their services and
spreading long-running jobs over a longer period of time. Do not abuse
this free service and ruin it for the rest of us.*

When the Sakai email data was spidered using this software, it produced
nearly a Gigabyte of data and took a number of runs on several days. The
file *README.txt* in the above ZIP may have instructions
as to how you can download a pre-spidered copy of the
*content.sqlite* file for a majority of the Sakai email
corpus so you don't have to spider for five days just to run the
programs. If you download the pre-spidered content, you should still run
the spidering process to catch up with more recent messages.

The first step is to spider the gmane repository. The base URL is
hard-coded in the *gmane.py* and is hard-coded to the
Sakai developer list. You can spider another repository by changing that
base url. Make sure to delete the *content.sqlite* file
if you switch the base url.

The *gmane.py* file operates as a responsible caching
spider in that it runs slowly and retrieves one mail message per second
so as to avoid getting throttled by gmane. It stores all of its data in
a database and can be interrupted and restarted as often as needed. It
may take many hours to pull all the data down. So you may need to
restart several times.

Here is a run of *gmane.py* retrieving the last five
messages of the Sakai developer list:

~~~~
How many messages:10
http://download.gmane.org/gmane.comp.cms.sakai.devel/51410/51411 9460
    nealcaidin@sakaifoundation.org 2013-04-05 re: [building ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51411/51412 3379
    samuelgutierrezjimenez@gmail.com 2013-04-06 re: [building ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51412/51413 9903
    da1@vt.edu 2013-04-05 [building sakai] melete 2.9 oracle ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51413/51414 349265
    m.shedid@elraed-it.com 2013-04-07 [building sakai] ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51414/51415 3481
    samuelgutierrezjimenez@gmail.com 2013-04-07 re: ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51415/51416 0

Does not start with From
~~~~

The program scans *content.sqlite* from one up to the
first message number not already spidered and starts spidering at that
message. It continues spidering until it has spidered the desired number
of messages or it reaches a page that does not appear to be a properly
formatted message.

Sometimes [gmane.org](gmane.org) is missing a message. Perhaps
administrators can delete messages or perhaps they get lost. If your
spider stops, and it seems it has hit a missing message, go into the
SQLite Manager and add a row with the missing id leaving all the other
fields blank and restart *gmane.py*. This will unstick
the spidering process and allow it to continue. These empty messages
will be ignored in the next phase of the process.

One nice thing is that once you have spidered all of the messages and
have them in *content.sqlite*, you can run
*gmane.py* again to get new messages as they are sent to
the list.

The *content.sqlite* data is pretty raw, with an
inefficient data model, and not compressed. This is intentional as it
allows you to look at *content.sqlite* in the SQLite
Manager to debug problems with the spidering process. It would be a bad
idea to run any queries against this database, as they would be quite
slow.

The second process is to run the program *gmodel.py*.
This program reads the raw data from *content.sqlite* and
produces a cleaned-up and well-modeled version of the data in the file
*index.sqlite*. This file will be much smaller (often 10X
smaller) than *content.sqlite* because it also compresses
the header and body text.

Each time *gmodel.py* runs it deletes and rebuilds
*index.sqlite*, allowing you to adjust its parameters and
edit the mapping tables in *content.sqlite* to tweak the
data cleaning process. This is a sample run of
*gmodel.py*. It prints a line out each time 250 mail
messages are processed so you can see some progress happening, as this
program may run for a while processing nearly a Gigabyte of mail data.

~~~~
Loaded allsenders 1588 and mapping 28 dns mapping 1
1 2005-12-08T23:34:30-06:00 ggolden22@mac.com
251 2005-12-22T10:03:20-08:00 tpamsler@ucdavis.edu
501 2006-01-12T11:17:34-05:00 lance@indiana.edu
751 2006-01-24T11:13:28-08:00 vrajgopalan@ucmerced.edu
...
~~~~

The *gmodel.py* program handles a number of data cleaning
tasks.

Domain names are truncated to two levels for .com, .org, .edu, and .net.
Other domain names are truncated to three levels. So si.umich.edu
becomes umich.edu and caret.cam.ac.uk becomes cam.ac.uk. Email addresses
are also forced to lower case, and some of the @gmane.org address like
the following

~~~~
arwhyte-63aXycvo3TyHXe+LvDLADg@public.gmane.org
~~~~
are converted to the real address whenever there is a matching real
email address elsewhere in the message corpus.

In the *mapping.sqlite* database there are two tables
that allow you to map both domain names and individual email addresses
that change over the lifetime of the email list. For example, Steve
Githens used the following email addresses as he changed jobs over the
life of the Sakai developer list:

~~~~
s-githens@northwestern.edu
sgithens@cam.ac.uk
swgithen@mtu.edu
~~~~

We can add two entries to the Mapping table in
*mapping.sqlite* so *gmodel.py* will map
all three to one address:

~~~~
s-githens@northwestern.edu ->  swgithen@mtu.edu
sgithens@cam.ac.uk -> swgithen@mtu.edu
~~~~

You can also make similar entries in the DNSMapping table if there are
multiple DNS names you want mapped to a single DNS. The following
mapping was added to the Sakai data:

~~~~
iupui.edu -> indiana.edu
~~~~

so all the accounts from the various Indiana University campuses are
tracked together.

You can rerun the *gmodel.py* over and over as you look
at the data, and add mappings to make the data cleaner and cleaner. When
you are done, you will have a nicely indexed version of the email in
*index.sqlite*. This is the file to use to do data
analysis. With this file, data analysis will be really quick.

The first, simplest data analysis is to determine "who sent the most
mail?" and "which organization sent the most mail"? This is done using
*gbasic.py*:

~~~~
How many to dump? 5
Loaded messages= 51330 subjects= 25033 senders= 1584

Top 5 Email list participants
steve.swinsburg@gmail.com 2657
azeckoski@unicon.net 1742
ieb@tfd.co.uk 1591
csev@umich.edu 1304
david.horwitz@uct.ac.za 1184

Top 5 Email list organizations
gmail.com 7339
umich.edu 6243
uct.ac.za 2451
indiana.edu 2258
unicon.net 2055
~~~~

Note how much more quickly *gbasic.py* runs compared to
*gmane.py* or even *gmodel.py*. They are
all working on the same data, but *gbasic.py* is using
the compressed and normalized data in *index.sqlite*. If
you have a lot of data to manage, a multistep process like the one in
this application may take a little longer to develop, but will save you
a lot of time when you really start to explore and visualize your data.

You can produce a simple visualization of the word frequency in the
subject lines in the file *gword.py*:

~~~~
Range of counts: 33229 129
Output written to gword.js
~~~~

This produces the file *gword.js* which you can visualize
using *gword.htm* to produce a word cloud similar to the
one at the beginning of this section.

A second visualization is produced by *gline.py*. It
computes email participation by organizations over time.

~~~~
Loaded messages= 51330 subjects= 25033 senders= 1584
Top 10 Oranizations
['gmail.com', 'umich.edu', 'uct.ac.za', 'indiana.edu',
'unicon.net', 'tfd.co.uk', 'berkeley.edu', 'longsight.com',
'stanford.edu', 'ox.ac.uk']
Output written to gline.js
~~~~

Its output is written to *gline.js* which is visualized
using *gline.htm*.

![Sakai Mail Activity by Organization](../images/mailorg)

This is a relatively complex and sophisticated application and has
features to do some real data retrieval, cleaning, and visualization.
